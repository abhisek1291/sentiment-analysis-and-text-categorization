# Text Categorization and Sentiment Analysis of Twitter Data

Generates the category and sentiment of tweets grouped by country and topic. The project implements two classifiers trained with the following training data.

# Training Data !

  - __Text Categorization__
    *  Training data are tweets taken from 6-7 users from the given category.
    *  For ex: For Sports category, we get the tweets from the following twitter users, __'espn', 'ESPNcricinfo', 'BBCSport', 'FOXSports', 'NBCSports'__
    *  All the list of users and the categorization data fetch logic can be found in the __categorization_training_data.py__ file.
  - __Sentiment Analysis__
    *  I have used the first 100,000 rows of stanford sentiment analysis dataset as the sentiment training data.

# Note:

  -  Since twitter limits the number of API accesses to 200 every 15 mins, there is a check on the number of API calls. If the number of API calls go over the limit, the application is put to sleep for 15 mins and then resumed. 
  -  Sometimes the Twitter API rejects a call once the rate limit is exceeded. I have added a logic in the __*catch*__ part to put the appliation to sleep for 15 mins and continue with the country that saw the exception.
  -  These can be found in the __tweets.py__ and __categorization_training_data.py__ files.

# Classification:

  - Data is classified into the following concept classes, as defined in the **concept_class.py** file:
    *  **'sports'**: 0,
    *  **'tech'**: 1,
    *  **'travel'**: 2,
    *  **'food'**: 3,
    *  **'politics'**: 4,
    *  **'automobiles'**: 5,
    *  **'weather'**: 6,
    *  **'music'**: 7,
    *  **'entertainment'**: 8,
    *  **'business'**: 9,
    *  **'shopping'**: 10,
    *  **'science'**: 11,
    *  **'health'**: 12,
    *  **'property'**: 13
  - SVMs and Naive Bayes are used for classification after the tweets are vectorized.
 
# Execution Sequence

Need to provide the following codes in the main.py. Can be generated by registering a new app at Twitter Developers.

ACCESS_TOKEN = 'ENTER_ACCESS_TOKEN_HERE'
ACCESS_SECRET = 'ENTER_ACCESS_SECRET_HERE'
CONSUMER_KEY = 'ENTER_CONSUMER_KEY_HERE'
CONSUMER_SECRET = 'ENTER_CONSUMER_SECRET_HERE'

  - called and controlled from __*main.py*__
    * Get the Country List (country.py)
    * Get the training data.
      * Categorization - __*categorization_training_data.py*__
      * Sentiment Analysis - __*sentiment_training_data.py*__
    * Prepare the Training data - __*categorization_process_traindata.py*__
    * Create and Train Classifiers
      * Categorization - __*categorization_classifier.py*__
      * Sentiment Analysis - __*sentiment_classifier.py*__
    * Once the classifiers are trained, write them to file using __*cPickle*__
    * Get realtime tweets for classification and write them to file - __*tweets.py*__ - takes a long time and generates large files.
    * Predict the Category and Sentiment - __*predict.py*__
 